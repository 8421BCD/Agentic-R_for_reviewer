import argparse
import os
from vllm import LLM, SamplingParams
import json
import re
from tqdm import tqdm
from typing import List
os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"

# **Deep Search Process:**
# The Deep Search is a multi-turn process where the LLM solves the original user question by iteratively thinking and searching. Each turn involves:
# 1. The LLM's internal Think Content (the reasoning process to solve the original user question), wrapped in <think> and </think>.
# 2. The Search Query to supplement the missing knowledge, wrapped in <search> and </search>.
# 3. The search results (Passages), wrapped in <information> and </information>.


def construct_prompt(initial_question: str, gold_answer_list: list, search_r1_full_trajectory: str, tokenizer) -> list:
    """
    Constructs the prompt for the Judge LLM (Qwen 2.5 72B) to infer sub-answers 
    based on the full deep search trajectory and gold answers.
    
    Args:
        initial_question (str): The original question asked by the user.
        gold_answer_list (list): List of possible gold answers for the question.
        search_r1_full_trajectory (str): The complete trace generated by the Search-R1 agent.

    Returns:
        list: The messages list (system and user content) for chat template application.
    """
    
    system_message = "You are an **Expert Trajectory Analyst and Answer Generator**. Your task is to analyze Agentic Search Agent Search-R1's complete reasoning trajectory for answering a given Question. Based on the Question and its Gold Answer(s), you must infer the **precise sub-answer** intended to be found by each generated search query in the trajectory."
    
    gold_answers_str = ", ".join([f'"{ans}"' for ans in gold_answer_list])

    prompt_template = f"""
**Search-R1's Multi-Turn Reasoning-Search Process:**
To answer a question, Search-R1 needs to conduct reasoning first every time it gets new information. After reasoning, if Search-R1 finds it lacks some knowledge, it will call a search engine by `<search> query </search>` and obtain the top searched results between `<information>` and `</information>`. Search-R1 can search as many times as it wants. If Search-R1 finds no further external knowledge needed, it can directly provide the answer inside `<answer>` and `</answer>` without detailed illustrations. For example, `<answer> xxx </answer>`.

Given the Question, Gold Answer(s) List and the Search-R1's Trajectory, you need to analyze the provided Trajectory step-by-step. For **every** search query, determine the **precise sub-answer** it was intended to find. The required sub-answer **MUST** be a direct fact or short piece of information that answers or resolves the `<search>` query and **CRITICALLY** must align with one of the provided Gold Answers.
If you can find such a sub-answer for each query, output it. If the sub-answer of a certain query cannot be reliably inferred (which often occurs when the Search-R1 ultimately predicts an incorrect answer), output "Not Sure".

**Strictly** wrap each generated sub-answer (or "Not Sure") using the tag `<sub-answer> </sub-answer>`. Generate at most one sub-answer for each query. Output the results according to the order of the corresponding queries in the trajectory. Output the answers **contiguously** and separated only by a single space. **Output only the answers wrapped with tags, nothing else.** For example: `<sub-answer> Beijing </sub-answer> <sub-answer> Not Sure </sub-answer>`. 

Here are the Question, Gold Answer(s) List and the Agent Trajectory:
**Question:**
{initial_question}

**Gold Answer(s) List:**
[{gold_answers_str}] 

**Agent Trajectory:**
{search_r1_full_trajectory}

"""
    
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt_template.strip()}
    ]
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    return prompt

def get_data_to_process(args):
    # load all training data
    with open(args.input_data_path, 'r') as f:
        all_data = json.load(f)
    # load processed training ids
    done_ids = set()
    if os.path.exists(args.result_path):
        with open(args.result_path, 'r') as f:
            for line in f:
                data = json.loads(line)
                done_ids.add(data['id'])
    data_to_process = [data for data in all_data if data['id'] not in done_ids]
    return data_to_process

def get_prompts(args, batch_data, tokenizer):
    batch_prompts = []
    for data in batch_data:
        question = data['question']
        golden_answers = data['golden_answers']
        # prefix = f'Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and it will return the top searched results between <information> and </information>. You can search as many times as your want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: {question}'
        # assert data['output']['prompt'].startswith(prefix)
        trajectory = data['output']['prompt'].split('<|im_start|>assistant\n')[-1]
        input_prompt = construct_prompt(question, golden_answers, trajectory, tokenizer)
        batch_prompts.append(input_prompt)                
    return batch_prompts


def extract_sub_answers(response_string: str) -> List[str]:
    regex_pattern = r'<sub-answer>(.*?)</sub-answer>'
    sub_answers = [match.strip() for match in re.findall(regex_pattern, response_string, re.IGNORECASE)]
    
    return sub_answers

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Running exp")
    parser.add_argument("--model_name_or_path", type=str)
    parser.add_argument("--num_gpus", type=int)
    parser.add_argument("--input_data_path", type=str)
    parser.add_argument("--result_path", type=str)
    parser.add_argument("--batch_size", type=int, default=512)
    args = parser.parse_args()

    os.makedirs(os.path.dirname(args.result_path), exist_ok=True)
    llm = LLM(args.model_name_or_path, 
                download_dir=os.getenv("HF_HOME"), 
                # max_model_len=4096,
                gpu_memory_utilization=0.9, 
                enforce_eager=False, 
                tensor_parallel_size=args.num_gpus
                )
    tokenizer = llm.get_tokenizer()

    data_to_process = get_data_to_process(args)

    # process the data
    for start_idx in tqdm(range(0, len(data_to_process), args.batch_size)):
        print(f'----------------- processing {start_idx} to {start_idx + args.batch_size}, totally {len(data_to_process)}....--------------')
        batch_data = data_to_process[start_idx: start_idx + args.batch_size]
        batch_prompts = get_prompts(args, batch_data, tokenizer)
        sampling_params = SamplingParams(
            temperature=0,
            max_tokens=1000,
            min_tokens=1,
        )
        print(batch_prompts[0])
        outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=True,)
        subanswer_list = [extract_sub_answers(output.outputs[0].text.strip()) for output in outputs]
        result_list = []
        with open(args.result_path, 'a') as f:
            for idx, data in enumerate(batch_data):
                if len(subanswer_list[idx]) == len(data['output']['retrieval_results']):
                    queries = [val['query'] for key, val in data['output']['retrieval_results'].items()]
                    result = {
                        "id": data['id'],
                        "question": data['question'],
                        "golden_answers": data['golden_answers'],
                        "query_subanswer_list": [{'query': query, 'subanswer': subanswer} for query, subanswer in zip(queries, subanswer_list[idx])],
                    }
                    result_list.append(result)
        with open(args.result_path, 'a') as f:
            for result in result_list:
                f.write(json.dumps(result) + '\n')

